{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPg1rDwqgt+dkRNU54XG7OM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Demo Pendulum using Deep Deterministic Policy Gradient (DDPG) RL algorithm\n"," - Demonstrates the classic inverted pendulum swingup problem that is based on the classic problem in control theory.\n"," - The system consists of a pendulum attached at one end to a fixed point, and the other end being free. The pendulum starts in a random position and the goal is to apply torque on the free end to swing it into an upright position, with its center of gravity right above the fixed point.\n"," - The problem/environment consist of 3 state/observation variables:\n","   - x Position of the pendulum => x = cos(theta) [-1 (min), 1 (max)]\n","   - y Position of the pendulum => y = sin(theta) [-1 (min), 1 (max)]\n","   - a Angular Velocity of the pendulum => [-8 (min), 8 (max)]\n"," - The action space of the problem/environment:\n","   - Torque of the pendulum [-2 (min), 2 (min)]\n","   - The action is in continuous space\n"," - Further details of the problem enviroment settings can found in this [Gymnasium documentation](https://gymnasium.farama.org/environments/classic_control/pendulum/)\n"," - The RL DDPG algorithm is based on the [Machin](https://machin.readthedocs.io/en/latest/about.html) RL library\n"," - Further details of the Machin RL framework can be found in this [repo](https://github.com/iffiX/machin/tree/master)\n"],"metadata":{"id":"BZCDJIDpBQTl"}},{"cell_type":"markdown","source":["#### Installs"],"metadata":{"id":"j0rqk0CiEtWk"}},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cx12uNCQA2fv","executionInfo":{"status":"ok","timestamp":1748749430458,"user_tz":-60,"elapsed":3356,"user":{"displayName":"adeidowu@hotmail.com","userId":"15313548625752705445"}},"outputId":"9253bb4c-72c8-4b2c-f306-086f93df0c92"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: machin in /usr/local/lib/python3.11/dist-packages (0.4.2)\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n","Requirement already satisfied: gymnasium[classic_control] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[classic_control]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[classic_control]) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[classic_control]) (0.0.4)\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[classic_control]) (2.6.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (from machin) (0.25.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from machin) (5.9.5)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from machin) (2.6.0+cu124)\n","Requirement already satisfied: pytorch-lightning>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from machin) (2.5.1.post0)\n","Requirement already satisfied: torchviz in /usr/local/lib/python3.11/dist-packages (from machin) (0.0.3)\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (from machin) (1.0.3)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from machin) (6.9.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from machin) (0.3.7)\n","Requirement already satisfied: GPUtil in /usr/local/lib/python3.11/dist-packages (from machin) (1.4.0)\n","Requirement already satisfied: tensorboardX in /usr/local/lib/python3.11/dist-packages (from machin) (2.6.2.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning>=1.2.0->machin) (4.67.1)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning>=1.2.0->machin) (6.0.2)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.2.0->machin) (2025.3.2)\n","Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning>=1.2.0->machin) (1.7.2)\n","Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning>=1.2.0->machin) (0.14.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (3.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->machin) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->machin) (1.3.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym->machin) (0.0.8)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy->machin) (4.4.2)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from moviepy->machin) (2.32.3)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy->machin) (0.1.12)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy->machin) (2.37.0)\n","Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy->machin) (0.6.0)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX->machin) (5.29.4)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from torchviz->machin) (0.20.3)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.2.0->machin) (3.11.15)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning>=1.2.0->machin) (75.2.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy->machin) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy->machin) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy->machin) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy->machin) (2025.4.26)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->machin) (3.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.2.0->machin) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.2.0->machin) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.2.0->machin) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.2.0->machin) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.2.0->machin) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.2.0->machin) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.2.0->machin) (1.20.0)\n","Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n"]}],"source":["!pip install gymnasium[classic_control] numpy matplotlib machin pyvirtualdisplay"]},{"cell_type":"markdown","source":["#### Specify imports"],"metadata":{"id":"f2KyedRLFi4K"}},{"cell_type":"code","source":["import gymnasium as gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.animation import FuncAnimation\n","from pyvirtualdisplay import Display\n","from IPython.display import HTML\n","from IPython import display\n","from machin.frame.algorithms import DDPGPer\n","from machin.utils.logging import default_logger as logger\n","from machin.model.nets import static_module_wrapper\n","import torch as t\n","import torch.nn as nn\n","from typing import Tuple, List, Any"],"metadata":{"id":"_9ztwm6XFClw","executionInfo":{"status":"ok","timestamp":1748750541051,"user_tz":-60,"elapsed":3,"user":{"displayName":"adeidowu@hotmail.com","userId":"15313548625752705445"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["#### Define global configurations"],"metadata":{"id":"UMrzwCLCGS0d"}},{"cell_type":"code","source":["# Pendulum RL environment configurations\n","PENDULUM_ENV = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n","STATE_DIM = 3\n","ACTION_DIM = 1\n","ACTION_RANGE = 2\n","MAX_EPISODES = 1000\n","MAX_STEPS = 200\n","NOISE_PARAMETER = (0, 0.2)\n","NOISE_MODE = \"normal\"\n","SOLVED_REWARS = -150\n","SOLVED_REPEAT = 5\n","EPISODE_UPDATE_FREQUENCY = 200\n","\n","# Rendering configs\n","DPI = 72\n","INTERVAL = 100 # ms"],"metadata":{"id":"jqcV_afbF_OV","executionInfo":{"status":"ok","timestamp":1748749508491,"user_tz":-60,"elapsed":12,"user":{"displayName":"adeidowu@hotmail.com","userId":"15313548625752705445"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["#### Utility class of helper functions"],"metadata":{"id":"7XWQYtfrmBER"}},{"cell_type":"code","source":["class Helpers:\n","  \"\"\"\n","  Utility class of helper functions\n","  \"\"\"\n","  @staticmethod\n","  def animateEnvironment(images: List[Any]):\n","    \"\"\"\n","    Animates the environment\n","    :param images: Images\n","    \"\"\"\n","    plt.figure(\n","        figsize=(images[0].shape[1]/DPI,images[0].shape[0]/DPI),\n","        dpi=DPI\n","        )\n","    patch = plt.imshow(images[0])\n","    plt.axis=('off')\n","    animate = lambda i: patch.set_data(images[i])\n","    ani = FuncAnimation(\n","        plt.gcf(),\n","        animate,\n","        frames=len(images),\n","        interval=INTERVAL)\n","    display.display(display.HTML(ani.to_jshtml()))\n","    plt.close()"],"metadata":{"id":"j4HPV-ldmCb8","executionInfo":{"status":"ok","timestamp":1748749543764,"user_tz":-60,"elapsed":46,"user":{"displayName":"adeidowu@hotmail.com","userId":"15313548625752705445"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["#### Solution steps:\n"," - Step 1: Define Actor network of the DDPG RL algorithm\n"," - Step 2: Define Critic network of the DDPG RL algorithm\n"," - Step 3: Implement the DDPG RL training loop\n"," - Step 4: Implement the RL traievaluation (with animation) policy"],"metadata":{"id":"eFqUHqu4H_wk"}},{"cell_type":"markdown","source":["##### STEP 1: Define Actor network of the DDPG RL algorithm[link text](https://)"],"metadata":{"id":"k8f47sCUJB91"}},{"cell_type":"code","source":["class Actor(nn.Module):\n","  \"\"\"\n","  Actor network of the DDPG RL algorithm\n","  \"\"\"\n","  def __init__(self, state_dim, action_dim, action_range):\n","      super().__init__()\n","\n","      self.fc1 = nn.Linear(state_dim, 16)\n","      self.fc2 = nn.Linear(16, 16)\n","      self.fc3 = nn.Linear(16, action_dim)\n","      self.action_range = action_range\n","\n","  def forward(self, state):\n","      a = t.relu(self.fc1(state))\n","      a = t.relu(self.fc2(a))\n","      a = t.tanh(self.fc3(a)) * self.action_range\n","      return a"],"metadata":{"id":"57Mr8qRuG7fx","executionInfo":{"status":"ok","timestamp":1748747019900,"user_tz":-60,"elapsed":26,"user":{"displayName":"adeidowu@hotmail.com","userId":"15313548625752705445"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["##### STEP 2: Define Critic network of the DDPG RL algorithm"],"metadata":{"id":"K0Afgt3fJUr4"}},{"cell_type":"code","source":["class Critic(nn.Module):\n","  \"\"\"\n","  Critic network of the DDPG RL algorithm\n","  \"\"\"\n","  def __init__(self, state_dim, action_dim):\n","      super().__init__()\n","\n","      self.fc1 = nn.Linear(state_dim + action_dim, 16)\n","      self.fc2 = nn.Linear(16, 16)\n","      self.fc3 = nn.Linear(16, 1)\n","\n","  def forward(self, state, action):\n","      state_action = t.cat([state, action], 1)\n","      q = t.relu(self.fc1(state_action))\n","      q = t.relu(self.fc2(q))\n","      q = self.fc3(q)\n","      return q\n"],"metadata":{"id":"w9gN54kLJL3j","executionInfo":{"status":"ok","timestamp":1748747019905,"user_tz":-60,"elapsed":4,"user":{"displayName":"adeidowu@hotmail.com","userId":"15313548625752705445"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["##### STEP 3: Implement the DDPG RL training loop"],"metadata":{"id":"5RWF3YspKB5U"}},{"cell_type":"code","source":["class DDPGAgentTrainingLoop:\n","  \"\"\"\n","  DDPG agent training loop\n","  \"\"\"\n","  def __init__(\n","      self,\n","      env=PENDULUM_ENV,\n","      n_episodes=MAX_EPISODES,\n","      max_steps=MAX_STEPS,\n","      noise_parameter=NOISE_PARAMETER,\n","      noise_mode=NOISE_MODE,\n","      state_dim=STATE_DIM,\n","      action_dim=ACTION_DIM,\n","      action_range=ACTION_RANGE\n","  ):\n","    \"\"\"\n","    Initializes the DQN agent training loop\n","    :param env: Environment\n","    :param n_episodes: Number of episodes\n","    :param max_steps: Maximum number of steps\n","    :param noise_parameter: Noise parameter\n","    :param noise_mode: Noise\n","    \"\"\"\n","    self.env = env\n","    self.n_episodes = n_episodes\n","    self.max_steps = max_steps\n","    self.noise_parameter = noise_parameter\n","    self.noise_mode = noise_mode\n","    self.state_dim = state_dim\n","    self.action_dim = action_dim\n","    self.action_range = action_range\n","    self.device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n","\n","    actor = Actor(self.state_dim, self.action_dim, self.action_range)\n","    self.actor = static_module_wrapper(actor, \"cpu\", \"cpu\")\n","    actor_t = Actor(self.state_dim, self.action_dim, self.action_range)\n","    self.actor_t = static_module_wrapper(actor_t, \"cpu\", \"cpu\")\n","\n","    critic = Critic(self.state_dim, self.action_dim)\n","    self.critic = static_module_wrapper(critic, \"cpu\", \"cpu\")\n","    critic_t = Critic(self.state_dim, self.action_dim)\n","    self.critic_t = static_module_wrapper(critic_t, \"cpu\", \"cpu\")\n","\n","    self.ddpg_per = DDPGPer(\n","        self.actor,\n","        self.actor_t,\n","        self.critic,\n","        self.critic_t,\n","        t.optim.Adam,\n","        nn.MSELoss(reduction=\"sum\")\n","    )\n","\n","    self.step, self.reward_fulfilled = 0, 0\n","    self.smoothed_total_reward = 0\n","\n","  def train(self):\n","    \"\"\"\n","    Trains the DQN agent\n","    \"\"\"\n","    all_rewards = []\n","\n","    for episode in range(1, self.n_episodes+1):\n","      # print(f\"Episode: {self.episode}\")\n","      total_reward = 0\n","      terminal = False\n","      self.step = 0\n","      state_0, _ = self.env.reset()\n","      state = t.tensor(state_0, dtype=t.float32).view(1, self.state_dim)\n","      tmp_observations = []\n","\n","      while not terminal and self.step <= self.max_steps:\n","        self.step += 1\n","        with t.no_grad():\n","            old_state = state\n","            # agent model inference\n","            action = self.ddpg_per.act_with_noise(\n","                {\"state\": old_state},\n","                noise_param=self.noise_parameter,\n","                mode=self.noise_mode\n","            )\n","            state, reward, terminal, _, _ = self.env.step(action.numpy())\n","            # print(f\"terminal: {terminal}\")\n","            state = t.tensor(state, dtype=t.float32).view(1, self.state_dim)\n","            total_reward += reward[0]\n","\n","            tmp_observations.append(\n","                {\n","                    \"state\": {\"state\": old_state},\n","                    \"action\": {\"action\": action},\n","                    \"next_state\": {\"state\": state},\n","                    \"reward\": reward[0],\n","                    \"terminal\": terminal or self.step == self.max_steps,\n","                }\n","              )\n","\n","      self.ddpg_per.store_episode(tmp_observations)\n","      # update, update more if episode is longer, else less\n","      if episode > 100:\n","          for _ in range(self.step):\n","              self.ddpg_per.update()\n","\n","      all_rewards.append(total_reward)\n","\n","      if episode % EPISODE_UPDATE_FREQUENCY == 0:\n","            print(f\"Episode: {episode}, Avg Reward: {np.mean(all_rewards[-EPISODE_UPDATE_FREQUENCY:]):.3f}\")\n","\n","    return self.ddpg_per, all_rewards\n","\n",""],"metadata":{"id":"emsYWBSzJfj_","executionInfo":{"status":"ok","timestamp":1748747236801,"user_tz":-60,"elapsed":3,"user":{"displayName":"adeidowu@hotmail.com","userId":"15313548625752705445"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["env = PENDULUM_ENV\n","agent = DDPGAgentTrainingLoop(env)\n","ddpg_net, rewards = agent.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nWZX6qorMGG8","executionInfo":{"status":"ok","timestamp":1748748726009,"user_tz":-60,"elapsed":1488618,"user":{"displayName":"adeidowu@hotmail.com","userId":"15313548625752705445"}},"outputId":"12b1e60a-1326-44eb-f5b9-3a97ec6305b5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[33m[2025-06-01 03:07:16,824] <WARNING>:default_logger:The reduction property of criterion is not 'none', automatically corrected.\u001b[0m\n","WARNING:default_logger:The reduction property of criterion is not 'none', automatically corrected.\n","\u001b[33m[2025-06-01 03:07:16,827] <WARNING>:default_logger:                \n","                You have not specified the i/o device of your model <class '__main__.Actor'>\n","                Automatically determined and set to: cpu\n","\n","                The framework is not responsible for any un-matching device issues \n","                caused by this operation.\u001b[0m\n","WARNING:default_logger:                \n","                You have not specified the i/o device of your model <class '__main__.Actor'>\n","                Automatically determined and set to: cpu\n","\n","                The framework is not responsible for any un-matching device issues \n","                caused by this operation.\n","\u001b[33m[2025-06-01 03:07:30,977] <WARNING>:default_logger:                \n","                You have not specified the i/o device of your model <class '__main__.Actor'>\n","                Automatically determined and set to: cpu\n","\n","                The framework is not responsible for any un-matching device issues \n","                caused by this operation.\u001b[0m\n","WARNING:default_logger:                \n","                You have not specified the i/o device of your model <class '__main__.Actor'>\n","                Automatically determined and set to: cpu\n","\n","                The framework is not responsible for any un-matching device issues \n","                caused by this operation.\n","\u001b[33m[2025-06-01 03:07:30,983] <WARNING>:default_logger:                \n","                You have not specified the i/o device of your model <class '__main__.Critic'>\n","                Automatically determined and set to: cpu\n","\n","                The framework is not responsible for any un-matching device issues \n","                caused by this operation.\u001b[0m\n","WARNING:default_logger:                \n","                You have not specified the i/o device of your model <class '__main__.Critic'>\n","                Automatically determined and set to: cpu\n","\n","                The framework is not responsible for any un-matching device issues \n","                caused by this operation.\n","\u001b[33m[2025-06-01 03:07:30,986] <WARNING>:default_logger:                \n","                You have not specified the i/o device of your model <class '__main__.Critic'>\n","                Automatically determined and set to: cpu\n","\n","                The framework is not responsible for any un-matching device issues \n","                caused by this operation.\u001b[0m\n","WARNING:default_logger:                \n","                You have not specified the i/o device of your model <class '__main__.Critic'>\n","                Automatically determined and set to: cpu\n","\n","                The framework is not responsible for any un-matching device issues \n","                caused by this operation.\n"]},{"output_type":"stream","name":"stdout","text":["Episode: 200, Avg Reward: -1269.731\n","Episode: 400, Avg Reward: -539.452\n","Episode: 600, Avg Reward: -235.538\n","Episode: 800, Avg Reward: -170.961\n","Episode: 1000, Avg Reward: -161.476\n"]}]},{"cell_type":"markdown","source":["#### Step 4: Implement the RL traievaluation (with animation) policy"],"metadata":{"id":"P0DOTh4Xjlp-"}},{"cell_type":"code","source":["class EvaluateDDPGAgent:\n","  \"\"\"\n","  Evaluate the DDPG RL agent using animation of the simulation runs\n","  \"\"\"\n","  def __init__(self, agent, env, n_episodes=10, max_steps=100):\n","    \"\"\"\n","    Constructor\n","    \"\"\"\n","    self.agent = agent\n","    self.env = env\n","    self.n_episodes = n_episodes\n","    self.max_steps = max_steps\n","    self.device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n","    self.display = Display(visible=0, size=(400, 400))\n","    self.display.start()\n","    self.trajectories = []\n","    self.images = []\n","\n","  def _evaluate(self):\n","    \"\"\"\n","    Evaluate the agent\n","    \"\"\"\n","    for ep in range(self.n_episodes):\n","      total_reward = 0\n","      terminal = False\n","      self.step = 0\n","      state_0, _ = self.env.reset()\n","      state = t.tensor(state_0, dtype=t.float32).view(1, self.agent.state_dim)\n","      tmp_observations = []\n","\n","      #while not done:\n","      for _ in range(self.max_steps):\n","        with t.no_grad():\n","            old_state = state\n","            # agent model inference\n","            action = self.agent.ddpg_per.act_with_noise(\n","                {\"state\": old_state},\n","                noise_param=self.agent.noise_parameter,\n","                mode=self.agent.noise_mode\n","            )\n","            state, reward, terminal, _, _ = self.env.step(action.numpy())\n","            state = t.tensor(state, dtype=t.float32).view(1, self.agent.state_dim)\n","            total_reward += reward[0]\n","\n","            tmp_observations.append(\n","                {\n","                    \"state\": {\"state\": old_state},\n","                    \"action\": {\"action\": action},\n","                    \"next_state\": {\"state\": state},\n","                    \"reward\": reward[0],\n","                    \"terminal\": terminal or self.step == self.max_steps,\n","                }\n","              )\n","\n","        self.images.append(self.env.render())\n","\n","        if terminal:\n","              break\n","\n","      self.env.close()\n","\n","  def run(self):\n","    \"\"\"\n","    Run the RL evaluation with animation\n","    \"\"\"\n","    self._evaluate()\n","    Helpers.animateEnvironment(self.images)"],"metadata":{"id":"jyneemewVyEK","executionInfo":{"status":"ok","timestamp":1748750412073,"user_tz":-60,"elapsed":13,"user":{"displayName":"adeidowu@hotmail.com","userId":"15313548625752705445"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["evaluate = EvaluateDDPGAgent(agent, env)\n","evaluate.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":670,"output_embedded_package_id":"1Yb62r_-GfYN60KD_j4uE-OLe5wNUR6lG"},"id":"rLP-Yoj2obLt","executionInfo":{"status":"ok","timestamp":1748750643983,"user_tz":-60,"elapsed":94613,"user":{"displayName":"adeidowu@hotmail.com","userId":"15313548625752705445"}},"outputId":"3ee71608-eda9-49ee-8a83-250af27d3f15"},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"pWpiM5aMojFz"},"execution_count":null,"outputs":[]}]}