{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a3494f-c17c-4c84-9667-5f5a715dcf88",
   "metadata": {},
   "source": [
    "#### Simple DQN RL Stock Trader (Part1)\n",
    " - Simple stock trading RL-based algo trading agaent using S & P 500 data\n",
    " - Trading actions are buy, sel or hold\n",
    " - Trading window is daily\n",
    " - References are:\n",
    " - References:\n",
    "    - Mnih, V. et al., \"Human-level control through deep reinforcement learning\", Nature, 2015.\n",
    "    - Moody, J., Saffell, M., \"Learning to trade via direct reinforcement\", IEEE, 2001.\n",
    "    - Gymnasium API documentation: https://gymnasium.farama.org/\n",
    "    - PyTorch documentation: https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bade9d8a-2873-4eca-bcbb-a6640d7bb976",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "736f0b48-7394-4684-93ac-27a0e13a5270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import deque\n",
    "import ta\n",
    "import random\n",
    "import os\n",
    "from typing import Tuple, List, Any, Dict\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac4227-c6b5-4045-ba70-f3240048ab2e",
   "metadata": {},
   "source": [
    "#### Define global constants and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6bdb764-8d4c-4b76-a172-8f8afcaff454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure RL computations are reproducible by setting the seed\n",
    "SEED_VALUE = 100\n",
    "\n",
    "# S & P 500 data configurations\n",
    "DATA_PATH = \"../data\"\n",
    "S_P_RAW_DATA_PATH = f\"{DATA_PATH}/s_and_p_raw_data_with_features.csv\"\n",
    "S_P_SCALED_DATA_PATH = f\"{DATA_PATH}/s_and_p_scaled_data_with_features.csv\"\n",
    "DATA_START_DATE=\"2010-01-01\"\n",
    "DATA_END_DATE=\"2020-01-01\"\n",
    "S_AND_P_YAHOO_TICKER = \"^GSPC\"\n",
    "DATA_BAR_TYPE = \"Close\"\n",
    "FEATURE_SMA_10 = \"SMA_10\"\n",
    "FEATURE_RSI = \"RSI\"\n",
    "FEATURE_MACD = \"MACD\"\n",
    "S_AND_P_DATA_COLUMNS = [DATA_BAR_TYPE, FEATURE_SMA_10, FEATURE_RSI, FEATURE_MACD]\n",
    "\n",
    "# Data partition configuration\n",
    "TEST_SPLIT_FACTOR=0.2\n",
    "\n",
    "# RL training configuration\n",
    "TRAINING_EPISODES_COUNT = 100\n",
    "TRAINING_AVERAGE_ROLLING_WINDOW = 50\n",
    "\n",
    "# RL validation/test configuration\n",
    "TEST_EPISODES_COUNT = 3\n",
    "\n",
    "# DQN agent hyper-parameter configurations\n",
    "REPLAY_EXPERIENCE_MEMORY_SIZE = 10_000\n",
    "LEARNING_RATE = 1e-3\n",
    "GAMMA = 0.95\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Model persistence configuration\n",
    "MODEL_FOLDER = \"../model\"\n",
    "MODEL_RESULTS_FOLDER = \"../results\"\n",
    "os.makedirs(MODEL_FOLDER, exist_ok=True)\n",
    "os.makedirs(MODEL_RESULTS_FOLDER, exist_ok=True)\n",
    "MODEL_FILE_PATH = f\"{MODEL_FOLDER}/simple_rl_agent_v1.pt\"\n",
    "TRAIN_REWARDS_FILE_PATH = f\"{MODEL_RESULTS_FOLDER}/simple_rl_agent_v1_train_rewards.csv\"\n",
    "TRAIN_AVERAGE_REWARDS_FILE_PATH = f\"{MODEL_RESULTS_FOLDER}/simple_rl_agent_v1_train_average_rewards.csv\"\n",
    "TEST_REWARDS_FILE_PATH = f\"{MODEL_RESULTS_FOLDER}/simple_rl_agent_v1_test_rewards.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbb3ee-92aa-49a3-836f-70dafd02f5a2",
   "metadata": {},
   "source": [
    "#### Define S & P 500 data provider component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c6920f-bcb1-4903-9d88-37f1eb8193e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProvider:\n",
    "    \"\"\"\n",
    "    Component used to provide the S & P 500 dataset\n",
    "    :param s_and_p_raw_data_path: File path of raw data\n",
    "    :param s_and_p_scaled_data_path: File path of scaled data\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        s_and_p_raw_data_path: str = S_P_RAW_DATA_PATH,\n",
    "        s_and_p_scaled_data_path: str = S_P_SCALED_DATA_PATH\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "        self._s_and_p_raw_data_path = s_and_p_raw_data_path\n",
    "        self._s_and_p_scaled_data_path = s_and_p_scaled_data_path\n",
    "        self._closing_price_raw_df = None\n",
    "        self._closing_price_raw = None\n",
    "        self._closing_price_scaled = None\n",
    "        self._closing_price_train = None\n",
    "        self._closing_price_test = None\n",
    "        self._data_scaler = None\n",
    "        self.features = None\n",
    "\n",
    "    def _featureEngineerData(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Feature engineer the S & P price data\n",
    "        :param df: Input dataframe\n",
    "        :return: Feature engineered data\n",
    "        \"\"\"\n",
    "        df[FEATURE_SMA_10] = ta.trend.sma_indicator(df[DATA_BAR_TYPE], window=10)\n",
    "        df[FEATURE_RSI] = ta.momentum.RSIIndicator(df[DATA_BAR_TYPE], window=14).rsi()\n",
    "        df[FEATURE_MACD] = ta.trend.macd_diff(df[DATA_BAR_TYPE])\n",
    "        df.dropna(inplace=True)\n",
    "        return df\n",
    "\n",
    "    def getData(self) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Gets the S & P closing price data\n",
    "        :return: Datasets\n",
    "        \"\"\"\n",
    "        if os.path.exists(self._s_and_p_raw_data_path):\n",
    "            print(f\"{self._s_and_p_raw_data_path} already exists in local file system (cache), ingesting the file locally\")\n",
    "            self._closing_price_raw_df = pd.read_csv(self._s_and_p_raw_data_path, index_col=None)\n",
    "        else:\n",
    "            print(f\"{self._s_and_p_raw_data_path} does not exists in local file system (cache), so ingesting the file from Yahoo Finance remote endpoint..\")\n",
    "            self._closing_price_raw_df = yf.download(S_AND_P_YAHOO_TICKER, start=DATA_START_DATE, end=DATA_END_DATE, multi_level_index=False)\n",
    "            self._closing_price_raw_df.to_csv(self._s_and_p_raw_data_path, index=False)             \n",
    "        if os.path.exists(self._s_and_p_scaled_data_path):\n",
    "            print(f\"{self._s_and_p_scaled_data_path} already exists in local file system (cache), ingesting the file locally\")\n",
    "            closing_price_scaled_df =  pd.read_csv(self._s_and_p_scaled_data_path, index_col=None)            \n",
    "            self._closing_price_with_features_scaled = closing_price_scaled_df[S_AND_P_DATA_COLUMNS].values\n",
    "        else:\n",
    "            print(f\"{self._s_and_p_scaled_data_path} does not exists in local file system (cache), so will recompute the data scaling..\")\n",
    "            close_prices_df = self._closing_price_raw_df[[DATA_BAR_TYPE]]\n",
    "            close_prices_with_features_df = self._featureEngineerData(close_prices_df)\n",
    "            \n",
    "            close_prices_with_features = close_prices_with_features_df.values\n",
    "            self._data_scaler = MinMaxScaler()\n",
    "            self._closing_price_with_features_scaled = self._data_scaler.fit_transform(close_prices_with_features)\n",
    "            closing_price_scaled_df = pd.DataFrame(self._closing_price_with_features_scaled, columns=[S_AND_P_DATA_COLUMNS])\n",
    "            closing_price_scaled_df.to_csv(self._s_and_p_scaled_data_path, index=False)\n",
    "        \n",
    "        self._partitionDataset()\n",
    "        \n",
    "        return  self._closing_price_raw_df, self._closing_price_with_features_scaled, self._closing_price_train, self._closing_price_test\n",
    "\n",
    "    def _partitionDataset(self, slit_fraction: float=TEST_SPLIT_FACTOR):\n",
    "        \"\"\"\n",
    "        Partitions data into training and test splits\n",
    "        :param slit_fraction: Split fraction\n",
    "        \"\"\"\n",
    "        prices = self._closing_price_with_features_scaled\n",
    "        split = int(len(prices) * 0.8)\n",
    "        self._closing_price_train, self._closing_price_test = prices[:split], prices[split:]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "510563fd-f0f4-403b-9534-d8a2069ef961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/s_and_p_raw_data_with_features.csv already exists in local file system (cache), ingesting the file locally\n",
      "../data/s_and_p_scaled_data_with_features.csv already exists in local file system (cache), ingesting the file locally\n"
     ]
    }
   ],
   "source": [
    "data_provider = DataProvider()\n",
    "close_data_raw_df, close_data_scaled, close_data_train, close_data_test = data_provider.getData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6eb8a-ab2a-4aac-ab4a-30feff9bf70c",
   "metadata": {},
   "source": [
    "#### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b646db95-9dab-40bb-a24a-aba47fc8b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Helpers:\n",
    "    \"\"\"\n",
    "    Helper utilities\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def appendTableRow(\n",
    "            df: pd.DataFrame,\n",
    "            row: pd.Series):\n",
    "        \"\"\"\n",
    "        :param df: Dataframe to append row to\n",
    "        :param row: Row to append\n",
    "        :return: New dataframe with appended row\n",
    "        \"\"\"\n",
    "        return pd.concat([\n",
    "            df,\n",
    "            pd.DataFrame([row], columns=row.index)]\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def createTable(\n",
    "        columns: List[str]\n",
    "        \n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a new data table\n",
    "        :param columns: Columns\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(\n",
    "            columns=columns\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def displayTable(\n",
    "        df: pd.DataFrame,\n",
    "        n_rows: int,\n",
    "        n_columns: int\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Displays sample rows of a data table\n",
    "        :param df: Data table\n",
    "        :param n_rows: Number of rows\n",
    "        \"\"\"\n",
    "        with pd.option_context(\"display.max_rows\", n_rows, \"display.max_columns\", n_columns,\n",
    "                       \"max_colwidth\", 100):\n",
    "            print(display(df[:n_rows]))\n",
    "\n",
    "    @staticmethod\n",
    "    def setSeeds(seed: int=SEED_VALUE):\n",
    "        \"\"\"\n",
    "        Sets the seed value for the computation to maintain reproducibility\n",
    "        :param seed: Seed value\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd6b712-48c7-4b58-be0d-8ef1c7665b59",
   "metadata": {},
   "source": [
    "#### Define the custom trading environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "002a228c-ebc1-46d7-aaa8-0d8d4d64169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    RL stock trading environment    \n",
    "    \"\"\"\n",
    "    def __init__(self, features):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.max_step = len(features) - 1\n",
    "        self.action_space = gym.spaces.Discrete(3)  # 0: Hold, 1: Buy, 2: Sell\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(features.shape[1],), dtype=np.float32)\n",
    "        self.stop_loss_threshold = 0.05  # 5% loss tolerance\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=SEED_VALUE, options=None):\n",
    "        \"\"\"\n",
    "        Resets the RL environment\n",
    "        :param seed: Seed\n",
    "        :param options: Options\n",
    "        \"\"\"\n",
    "        self.balance = 1.0\n",
    "        self.shares_held = 0\n",
    "        self.buy_price = 0\n",
    "        self.net_worth = 1.0\n",
    "        self.current_step = 0\n",
    "        self.n_hold = 0\n",
    "        self.n_buy = 0\n",
    "        self.n_sell = 0\n",
    "        return self._getObservations(), {}\n",
    "\n",
    "    def _getObservations(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gets the RL environment onservations\n",
    "        :param observations: Observations\n",
    "        \"\"\"\n",
    "        return np.array([self.features[self.current_step]], dtype=np.float32)\n",
    "\n",
    "    def _getInfos(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Gets the RL infos\n",
    "        :return: RL infos\n",
    "        \"\"\"\n",
    "        info = {\n",
    "            \"shares_held\": self.shares_held,\n",
    "            \"balance\": self.balance,\n",
    "            \"n_hold\": self.n_hold,\n",
    "            \"n_sell\": self.n_sell,\n",
    "            \"n_buy\": self.n_buy,\n",
    "    \n",
    "        }\n",
    "        return info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Step function\n",
    "        :param action: Action taken by the agent\n",
    "        :return: Observations, reward, done, truncated and infos\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        current_price = self.features[self.current_step][0]\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1 and self.balance > 0:  # Buy\n",
    "            self.n_buy += 1\n",
    "            self.shares_held = self.balance / current_price\n",
    "            self.buy_price = current_price\n",
    "            self.balance = 0\n",
    "        elif action == 2 and self.shares_held > 0:  # Sell\n",
    "            self.n_sell += 1\n",
    "            self.balance = self.shares_held * current_price\n",
    "            self.shares_held = 0\n",
    "        else:  # Hold\n",
    "            self.n_hold += 1\n",
    "\n",
    "        # Risk Management: Apply stop-loss\n",
    "        if self.shares_held > 0 and current_price < self.buy_price * (1 - self.stop_loss_threshold):\n",
    "            self.balance = self.shares_held * current_price\n",
    "            self.shares_held = 0\n",
    "        \n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_step:\n",
    "            done = True\n",
    "\n",
    "        self.net_worth = self.balance + self.shares_held * self.features[self.current_step][0]\n",
    "        reward = self.net_worth - 1.0\n",
    "       \n",
    "        return self._getObservations(), reward, done, False, self._getInfos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc089fa-c8a8-4588-8119-e282e851f85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adeid\\AppData\\Local\\Temp\\ipykernel_8004\\2024178422.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat([\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>state</th>\n",
       "      <th>next_state</th>\n",
       "      <th>reward</th>\n",
       "      <th>done</th>\n",
       "      <th>truncated</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[[0.038526405, 0.016702535, 0.5397403, 0.70205945]]</td>\n",
       "      <td>[[0.032478876, 0.018448511, 0.4524713, 0.6834729]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'shares_held': 0, 'balance': 1.0, 'n_hold': 1, 'n_sell': 0, 'n_buy': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[0.032478876, 0.018448511, 0.4524713, 0.6834729]]</td>\n",
       "      <td>[[0.037277207, 0.02004968, 0.5172383, 0.6810519]]</td>\n",
       "      <td>0.147737</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'shares_held': 30.789242992436304, 'balance': 0, 'n_hold': 1, 'n_sell': 0, 'n_buy': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[[0.037277207, 0.02004968, 0.5172383, 0.6810519]]</td>\n",
       "      <td>[[0.036239956, 0.021654999, 0.50217265, 0.67293316]]</td>\n",
       "      <td>0.147737</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'shares_held': 0, 'balance': 1.1477370283268697, 'n_hold': 1, 'n_sell': 1, 'n_buy': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[[0.036239956, 0.021654999, 0.50217265, 0.67293316]]</td>\n",
       "      <td>[[0.03693898, 0.022854954, 0.5120296, 0.6661433]]</td>\n",
       "      <td>0.147737</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'shares_held': 0, 'balance': 1.1477370283268697, 'n_hold': 2, 'n_sell': 1, 'n_buy': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[[0.03693898, 0.022854954, 0.5120296, 0.6661433]]</td>\n",
       "      <td>[[0.041998856, 0.024708841, 0.58015066, 0.67210793]]</td>\n",
       "      <td>0.147737</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'shares_held': 0, 'balance': 1.1477370283268697, 'n_hold': 3, 'n_sell': 1, 'n_buy': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[[0.041998856, 0.024708841, 0.58015066, 0.67210793]]</td>\n",
       "      <td>[[0.043171424, 0.02578982, 0.5950677, 0.6750699]]</td>\n",
       "      <td>0.147737</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'shares_held': 0, 'balance': 1.1477370283268697, 'n_hold': 4, 'n_sell': 1, 'n_buy': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>[[0.043171424, 0.02578982, 0.5950677, 0.6750699]]</td>\n",
       "      <td>[[0.043387882, 0.02667895, 0.59794664, 0.6732059]]</td>\n",
       "      <td>0.147737</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'shares_held': 0, 'balance': 1.1477370283268697, 'n_hold': 5, 'n_sell': 1, 'n_buy': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>[[0.043387882, 0.02667895, 0.59794664, 0.6732059]]</td>\n",
       "      <td>[[0.045272905, 0.02742696, 0.6236484, 0.6727061]]</td>\n",
       "      <td>0.197602</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'shares_held': 26.45294062627436, 'balance': 0, 'n_hold': 5, 'n_sell': 1, 'n_buy': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>[[0.045272905, 0.02742696, 0.6236484, 0.6727061]]</td>\n",
       "      <td>[[0.052366663, 0.02878878, 0.7083689, 0.68692416]]</td>\n",
       "      <td>0.197602</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'shares_held': 0, 'balance': 1.1976015042042776, 'n_hold': 5, 'n_sell': 2, 'n_buy': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>[[0.052366663, 0.02878878, 0.7083689, 0.68692416]]</td>\n",
       "      <td>[[0.052276492, 0.030194877, 0.7064409, 0.69001627]]</td>\n",
       "      <td>0.197602</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'shares_held': 0, 'balance': 1.1976015042042776, 'n_hold': 6, 'n_sell': 2, 'n_buy': 2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  action                                                 state  \\\n",
       "0      2   [[0.038526405, 0.016702535, 0.5397403, 0.70205945]]   \n",
       "1      1    [[0.032478876, 0.018448511, 0.4524713, 0.6834729]]   \n",
       "2      2     [[0.037277207, 0.02004968, 0.5172383, 0.6810519]]   \n",
       "3      0  [[0.036239956, 0.021654999, 0.50217265, 0.67293316]]   \n",
       "4      0     [[0.03693898, 0.022854954, 0.5120296, 0.6661433]]   \n",
       "5      0  [[0.041998856, 0.024708841, 0.58015066, 0.67210793]]   \n",
       "6      2     [[0.043171424, 0.02578982, 0.5950677, 0.6750699]]   \n",
       "7      1    [[0.043387882, 0.02667895, 0.59794664, 0.6732059]]   \n",
       "8      2     [[0.045272905, 0.02742696, 0.6236484, 0.6727061]]   \n",
       "9      0    [[0.052366663, 0.02878878, 0.7083689, 0.68692416]]   \n",
       "\n",
       "                                             next_state    reward   done  \\\n",
       "0    [[0.032478876, 0.018448511, 0.4524713, 0.6834729]]  0.000000  False   \n",
       "1     [[0.037277207, 0.02004968, 0.5172383, 0.6810519]]  0.147737  False   \n",
       "2  [[0.036239956, 0.021654999, 0.50217265, 0.67293316]]  0.147737  False   \n",
       "3     [[0.03693898, 0.022854954, 0.5120296, 0.6661433]]  0.147737  False   \n",
       "4  [[0.041998856, 0.024708841, 0.58015066, 0.67210793]]  0.147737  False   \n",
       "5     [[0.043171424, 0.02578982, 0.5950677, 0.6750699]]  0.147737  False   \n",
       "6    [[0.043387882, 0.02667895, 0.59794664, 0.6732059]]  0.147737  False   \n",
       "7     [[0.045272905, 0.02742696, 0.6236484, 0.6727061]]  0.197602  False   \n",
       "8    [[0.052366663, 0.02878878, 0.7083689, 0.68692416]]  0.197602  False   \n",
       "9   [[0.052276492, 0.030194877, 0.7064409, 0.69001627]]  0.197602  False   \n",
       "\n",
       "  truncated  \\\n",
       "0     False   \n",
       "1     False   \n",
       "2     False   \n",
       "3     False   \n",
       "4     False   \n",
       "5     False   \n",
       "6     False   \n",
       "7     False   \n",
       "8     False   \n",
       "9     False   \n",
       "\n",
       "                                                                                      info  \n",
       "0                 {'shares_held': 0, 'balance': 1.0, 'n_hold': 1, 'n_sell': 0, 'n_buy': 0}  \n",
       "1  {'shares_held': 30.789242992436304, 'balance': 0, 'n_hold': 1, 'n_sell': 0, 'n_buy': 1}  \n",
       "2  {'shares_held': 0, 'balance': 1.1477370283268697, 'n_hold': 1, 'n_sell': 1, 'n_buy': 1}  \n",
       "3  {'shares_held': 0, 'balance': 1.1477370283268697, 'n_hold': 2, 'n_sell': 1, 'n_buy': 1}  \n",
       "4  {'shares_held': 0, 'balance': 1.1477370283268697, 'n_hold': 3, 'n_sell': 1, 'n_buy': 1}  \n",
       "5  {'shares_held': 0, 'balance': 1.1477370283268697, 'n_hold': 4, 'n_sell': 1, 'n_buy': 1}  \n",
       "6  {'shares_held': 0, 'balance': 1.1477370283268697, 'n_hold': 5, 'n_sell': 1, 'n_buy': 1}  \n",
       "7   {'shares_held': 26.45294062627436, 'balance': 0, 'n_hold': 5, 'n_sell': 1, 'n_buy': 2}  \n",
       "8  {'shares_held': 0, 'balance': 1.1976015042042776, 'n_hold': 5, 'n_sell': 2, 'n_buy': 2}  \n",
       "9  {'shares_held': 0, 'balance': 1.1976015042042776, 'n_hold': 6, 'n_sell': 2, 'n_buy': 2}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def testRLWithRandomAgent():\n",
    "    \"\"\"\n",
    "    Test the environment with a random RL agent\n",
    "    \"\"\"\n",
    "    columns = [\"action\", \"state\", \"next_state\", \"reward\", \"done\", \"truncated\", \"info\"]\n",
    "    results_df = Helpers.createTable(columns=columns)\n",
    "    n_episodes = 10\n",
    "    env = TradingEnv(features=close_data_train)\n",
    "    assert env != None, \"Incorrect env constructed!!\"\n",
    "    state, info = env.reset()    \n",
    "    for i in range(n_episodes):\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        new_row = pd.Series(\n",
    "            {\n",
    "                \"action\": action,\n",
    "                \"state\": state,\n",
    "                \"next_state\": next_state,\n",
    "                \"reward\": reward,\n",
    "                \"done\": done,\n",
    "                \"truncated\": truncated,\n",
    "                \"info\": info,\n",
    "            })\n",
    "        results_df = Helpers.appendTableRow(results_df, new_row)\n",
    "        state = next_state\n",
    "    Helpers.displayTable(results_df, n_rows=10, n_columns=len(columns))\n",
    "\n",
    "testRLWithRandomAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf10c124-7bc8-4094-89d4-7b0e779104f3",
   "metadata": {},
   "source": [
    "#### Define the Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "529d4ea6-1d77-42ea-b364-4b03cfe57616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Specification for the DQN network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param input_dim: Input dimension\n",
    "        :param output_dim: Output dimension\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Neural net forward pass\n",
    "        :param x: Input\n",
    "        \"\"\"\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d1e55-704e-4b25-882f-feef9b5bb816",
   "metadata": {},
   "source": [
    "#### RL DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24bc0f16-d3f6-4b10-99ac-c6ebd73e1883",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    DQN RL agent\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_dim: int, \n",
    "        action_dim: int):\n",
    "        \"\"\"\n",
    "        Cnstructor\n",
    "        :param state_dim: State dimension\n",
    "        :param action_dim: Action dimension\n",
    "        \"\"\"\n",
    "        self.model = DQN(state_dim, action_dim)\n",
    "        self.target_model = DQN(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.memory = deque(maxlen=REPLAY_EXPERIENCE_MEMORY_SIZE)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 64\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Records the RL experience trajectories\n",
    "        :param state: State\n",
    "        :param action: Action\n",
    "        :param reward: Reward\n",
    "        :param next_state: Next state\n",
    "        :param done: Done\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(\n",
    "        self, \n",
    "        state: np.ndarray\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Invokes the agents 'act'\n",
    "        :param state: State\n",
    "        :return: action\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(3)\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state_tensor)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"\n",
    "        Experience replay\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            next_state_tensor = torch.FloatTensor(next_state)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * torch.max(self.target_model(next_state_tensor)).item()\n",
    "\n",
    "            target_f = self.model(state_tensor)\n",
    "            target_f = target_f.clone().detach()\n",
    "            target_f[0, action] = target\n",
    "\n",
    "            self.model.train()\n",
    "            output = self.model(state_tensor)\n",
    "            loss = self.criterion(output, target_f)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"\n",
    "        Updates the network model\n",
    "        \"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9d4d17-ccd8-4fb4-8deb-4da1ebb8b765",
   "metadata": {},
   "source": [
    "#### Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f577f20-07cf-4384-ac50-f99e31c3b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAgent:\n",
    "    \"\"\"\n",
    "    Component used to train the RL agent\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        price_train_data: np.ndarray,\n",
    "        n_episodes: int=TRAINING_EPISODES_COUNT\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param price_train_data: Train data\n",
    "        :param n_episodes: Number of episodes for RL validation\n",
    "        \"\"\"\n",
    "        self.n_episodes = n_episodes\n",
    "        self.price_train_data = price_train_data\n",
    "        self.train_env = TradingEnv(self.price_train_data)\n",
    "        self.agent = DQNAgent(state_dim=self.price_train_data.shape[1], action_dim=3)\n",
    "        self.rewards = []\n",
    "        self.rewards_average = []\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the RL agent training cycle\n",
    "        \"\"\"\n",
    "        for episodes in tqdm(range(self.n_episodes), desc=\"Episodes\"):\n",
    "            state, _ = self.train_env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            for _ in range(len(self.price_train_data)):\n",
    "                action = self.agent.act(state)\n",
    "                next_state, reward, done, _, _ = self.train_env.step(action)\n",
    "                self.agent.remember(state, action, reward, next_state, done)\n",
    "                self.agent.replay()\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                self.rewards.append(total_reward)\n",
    "        \n",
    "                if done: \n",
    "                    average_reward = sum(self.rewards[-TRAINING_AVERAGE_ROLLING_WINDOW:]) / TRAINING_AVERAGE_ROLLING_WINDOW\n",
    "                    self.rewards_average.append(average_reward)\n",
    "                    break\n",
    "        \n",
    "            self.agent.update_target_model()\n",
    "            \n",
    "            #print(f\"Epoch {e+1}/{epochs}, Total Profit: {total_reward:.4f}, Epsilon: {self.agent.epsilon:.4f}\")\n",
    "\n",
    "        self.saveModel()\n",
    "\n",
    "    def saveModel(self):\n",
    "        \"\"\"\n",
    "        Save the neural net model\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if os.path.exists(MODEL_FOLDER):\n",
    "            torch.save(self.agent.model, MODEL_FILE_PATH)\n",
    "            rewards_df = pd.DataFrame(self.rewards, columns=[\"rewards\"])\n",
    "            rewards_average_df = pd.DataFrame(self.rewards_average, columns=[\"average_rewards\"])\n",
    "            rewards_df.to_csv(TRAIN_REWARDS_FILE_PATH, index=False)\n",
    "            rewards_average_df.to_csv(TRAIN_AVERAGE_REWARDS_FILE_PATH, index=False)\n",
    "        else:\n",
    "            print(f\"The folder {MODEL_FOLDER} does not exist and the model could not be saved!!\")\n",
    "\n",
    "    def loadModel(self) -> DQN:\n",
    "        \"\"\"\n",
    "        Loads the neural net model\n",
    "        :return: Loaded model\n",
    "        \"\"\"\n",
    "        model = None\n",
    "        if os.path.exist(MODEL_FILE_PATH):\n",
    "            model = torch.load(MODEL_FILE_PATH)\n",
    "        else:\n",
    "            print(f\"The path {MODEL_FILE_PATH} does not exist and the model could not be loaded!!\")\n",
    "        return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20de9d97-afeb-467b-9a44-e9eae6bb0b23",
   "metadata": {},
   "source": [
    "#### Run the RL training cycle.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9b9f7-62a7-4046-9938-5d8b64df4672",
   "metadata": {},
   "source": [
    "#### Validate/Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd74f92b-d2c3-44f2-9809-4e60c4f5ce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes:   2%|██                                                                                                       | 2/100 [09:09<7:45:31, 285.02s/it]C:\\Users\\adeid\\AppData\\Local\\Temp\\ipykernel_8004\\4154127969.py:66: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  self.shares_held = self.balance / current_price\n",
      "Episodes:  44%|████████████████████████████████████████████▉                                                         | 44/100 [6:55:10<4:56:23, 317.57s/it]"
     ]
    }
   ],
   "source": [
    "Helpers.setSeeds()\n",
    "# train_agent = TrainAgent(price_train_data=close_data_train[:50])\n",
    "train_agent = TrainAgent(price_train_data=close_data_train)\n",
    "train_agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1340aa67-f319-46e7-ba7d-8be3806aad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidateAgent:\n",
    "    \"\"\"\n",
    "    Component used to validate/test the RL agent\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        price_test_data: np.ndarray,\n",
    "        n_episodes: int=TEST_EPISODES_COUNT\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param price_test_data: Test data\n",
    "        :param n_episodes: Number of episodes for RL validation/test\n",
    "        \"\"\"\n",
    "        self.n_episodes = n_episodes\n",
    "        self.price_test_data = price_test_data\n",
    "        self.test_env = TradingEnv(self.price_train_data)\n",
    "        self.agent = DQNAgent(state_dim=1, action_dim=3)\n",
    "        self.reward = []\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the RL agent validation/test cycle\n",
    "        \"\"\"\n",
    "        self.agent.epsilon = 0.0  # Turn off exploration\n",
    "        for episodes in range(self.n_episodes):\n",
    "            state, _ = self.test_env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.agent.act(state)\n",
    "                next_state, reward, done, _, _ = self.test_env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            self.rewards.append(total_reward)\n",
    "            print(f\"Epoch {e+1}/{epochs}, Total Profit: {total_reward:.4f}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64174cf7-368d-4382-80b4-aaf7052769c2",
   "metadata": {},
   "source": [
    "#### Report RL performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec946afe-839f-4745-acf9-9da62df27157",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportRLPerformance:\n",
    "    \"\"\"\n",
    "    Component used to report the RL performance\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_rewards: List[float],\n",
    "        train_rewards_average: List[float]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param train_rewards: Training rewards\n",
    "        :param train_rewards_average: Validation rewards\n",
    "        \"\"\"\n",
    "        self.train_rewards = train_rewards\n",
    "        self.train_rewards_average = train_rewards_average\n",
    "\n",
    "    def plotRewardCurves(\n",
    "        self,\n",
    "        rewards: List[float],\n",
    "        plot_type: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots the reward curves\n",
    "        :param rewards: Rewards\n",
    "        :param plot_type: Plot type\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        plt.plot(rewards)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Total Profit\")\n",
    "        plt.title(f\"{plot_type} Reward Progress\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plotTrainingRewardCurves(\n",
    "        self,\n",
    "        plot_type: str = \"Training\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots the training reward curves\n",
    "        :param plot_type: Plot type\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.plotRewardCurves(self.train_rewards, plot_type=plot_type)\n",
    "\n",
    "    def plotsmoothedTrainingRewardCurves(\n",
    "        self,\n",
    "        plot_type: str = \"Smoothed Training\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots the training reward curves\n",
    "        :param plot_type: Plot type\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.plotRewardCurves(self.train_rewards_average, plot_type=plot_type)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ffbb0c-fa12-41e0-8e9e-b6c430062d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reporter = ReportRLPerformance(train_rewards = train_agent.rewards, train_rewards_average=train_agent.rewards_average)\n",
    "reporter.plotTrainingRewardCurves()\n",
    "reporter.plotsmoothedTrainingRewardCurves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81572ba8-b3ab-416d-b97e-6dafdc9f433e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ba029-4759-4fd8-9a0e-072828fb3243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358872d2-b4b5-40cb-87e7-f1e8565489f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128cb71f-d99a-4e85-85fe-8f67673825c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07859cff-6d4e-4cd7-81fd-e2669d150680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "    trewards = []\n",
    "    for _ in range(10):\n",
    "        \n",
    "        treward = _ + 1\n",
    "        trewards.append(treward)\n",
    "        print(trewards)\n",
    "\n",
    "\n",
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5bae5c-0805-4641-8d27-4972579e675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = list(range(100))\n",
    "t[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750fd25-ee3c-4389-a393-502ebcd9aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame({\"prices\": range(100)})\n",
    "df_1.rolling(10).sum().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544cdd5c-4bd7-4f53-83b2-7f712b13e6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.11) rl_env",
   "language": "python",
   "name": "rl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
