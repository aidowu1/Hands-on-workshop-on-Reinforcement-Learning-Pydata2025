2025-06-04 09:37:23,267 - INFO - ppo_algorithm.py:__init__ - : Start of Reinforcement learning for environment: PENDULUM-V1 in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\ppo_algorithm.py:48
2025-06-04 09:37:23,267 - INFO - ppo_algorithm.py:__init__ - : This RL environment uses a PPO RL algorithm agent in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\ppo_algorithm.py:49
2025-06-04 09:37:23,268 - INFO - base_algorithms.py:hyper_parameters - : Training is using non-tuned hyperparameters... in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\base_algorithms.py:116
2025-06-04 09:37:23,268 - INFO - base_algorithms.py:hyper_parameters - : Hyperparameters are:
{'batch_size': 64, 'gamma': 0.99, 'gae_lambda': 0.9, 'ent_coef': 0.0, 'sde_sample_freq': 4, 'max_grad_norm': 0.5, 'vf_coef': 0.5, 'learning_rate': 3e-05, 'use_sde': True, 'policy_kwargs': {'log_std_init': -2.7, 'ortho_init': False, 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'net_arch': {'pi': [256, 256], 'vf': [256, 256]}}} in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\base_algorithms.py:117
2025-06-04 09:42:04,333 - INFO - ppo_algorithm.py:__init__ - : Start of Reinforcement learning for environment: PENDULUM-V1 in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\ppo_algorithm.py:48
2025-06-04 09:42:04,333 - INFO - ppo_algorithm.py:__init__ - : This RL environment uses a PPO RL algorithm agent in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\ppo_algorithm.py:49
2025-06-04 09:42:04,334 - INFO - base_algorithms.py:hyper_parameters - : Training is using non-tuned hyperparameters... in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\base_algorithms.py:116
2025-06-04 09:42:04,334 - INFO - base_algorithms.py:hyper_parameters - : Hyperparameters are:
{'batch_size': 64, 'gamma': 0.99, 'gae_lambda': 0.9, 'ent_coef': 0.0, 'sde_sample_freq': 4, 'max_grad_norm': 0.5, 'vf_coef': 0.5, 'learning_rate': 3e-05, 'use_sde': True, 'policy_kwargs': {'log_std_init': -2.7, 'ortho_init': False, 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'net_arch': {'pi': [256, 256], 'vf': [256, 256]}}} in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\base_algorithms.py:117
2025-06-04 09:42:27,999 - INFO - ppo_algorithm.py:__init__ - : Start of Reinforcement learning for environment: PENDULUM-V1 in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\ppo_algorithm.py:48
2025-06-04 09:42:27,999 - INFO - ppo_algorithm.py:__init__ - : This RL environment uses a PPO RL algorithm agent in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\ppo_algorithm.py:49
2025-06-04 09:42:27,999 - INFO - base_algorithms.py:hyper_parameters - : Training is using non-tuned hyperparameters... in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\base_algorithms.py:116
2025-06-04 09:42:28,000 - INFO - base_algorithms.py:hyper_parameters - : Hyperparameters are:
{'batch_size': 64, 'gamma': 0.99, 'gae_lambda': 0.9, 'ent_coef': 0.0, 'sde_sample_freq': 4, 'max_grad_norm': 0.5, 'vf_coef': 0.5, 'learning_rate': 3e-05, 'use_sde': True, 'policy_kwargs': {'log_std_init': -2.7, 'ortho_init': False, 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'net_arch': {'pi': [256, 256], 'vf': [256, 256]}}} in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\base_algorithms.py:117
2025-06-04 09:48:36,769 - INFO - ppo_algorithm.py:__init__ - : Start of Reinforcement learning for environment: PENDULUM-V1 in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\ppo_algorithm.py:48
2025-06-04 09:48:36,769 - INFO - ppo_algorithm.py:__init__ - : This RL environment uses a PPO RL algorithm agent in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\ppo_algorithm.py:49
2025-06-04 09:48:36,770 - INFO - base_algorithms.py:hyper_parameters - : Training is using non-tuned hyperparameters... in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\base_algorithms.py:116
2025-06-04 09:48:36,770 - INFO - base_algorithms.py:hyper_parameters - : Hyperparameters are:
{'batch_size': 64, 'gamma': 0.99, 'gae_lambda': 0.9, 'ent_coef': 0.0, 'sde_sample_freq': 4, 'max_grad_norm': 0.5, 'vf_coef': 0.5, 'learning_rate': 3e-05, 'use_sde': True, 'policy_kwargs': {'log_std_init': -2.7, 'ortho_init': False, 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'net_arch': {'pi': [256, 256], 'vf': [256, 256]}}} in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\base_algorithms.py:117
2025-06-04 09:51:14,039 - INFO - ppo_algorithm.py:__init__ - : Start of Reinforcement learning for environment: PENDULUM-V1 in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\ppo_algorithm.py:48
2025-06-04 09:51:14,039 - INFO - ppo_algorithm.py:__init__ - : This RL environment uses a PPO RL algorithm agent in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\ppo_algorithm.py:49
2025-06-04 09:51:14,039 - INFO - base_algorithms.py:hyper_parameters - : Training is using non-tuned hyperparameters... in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\base_algorithms.py:116
2025-06-04 09:51:14,040 - INFO - base_algorithms.py:hyper_parameters - : Hyperparameters are:
{'batch_size': 64, 'gamma': 0.99, 'gae_lambda': 0.9, 'ent_coef': 0.0, 'sde_sample_freq': 4, 'max_grad_norm': 0.5, 'vf_coef': 0.5, 'learning_rate': 3e-05, 'policy_kwargs': {'log_std_init': -2.7, 'ortho_init': False, 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'net_arch': {'pi': [256, 256], 'vf': [256, 256]}}} in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\base_algorithms.py:117
2025-06-04 09:51:16,257 - INFO - rl_algo_trading_agent.py:train - : Training RL algo trading agent with the RLAgorithmType.ppo algorithm in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_agents\rl_algo_trading_agent.py:39
2025-06-04 09:51:16,263 - INFO - save_on_base_reward_callback.py:_init_callback - : Saving trained RL model to ./logs/Pendulum-v1_ppo\best_model in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:47
2025-06-04 09:51:20,071 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 09:51:23,015 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 09:51:47,418 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 09:51:52,224 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 09:51:57,030 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 09:52:25,301 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 09:52:34,872 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 09:52:39,665 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 09:52:44,489 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 09:53:02,340 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 21:21:11,931 - INFO - ppo_algorithm.py:__init__ - : Start of Reinforcement learning for environment: PENDULUM-V1 in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\ppo_algorithm.py:48
2025-06-04 21:21:11,931 - INFO - ppo_algorithm.py:__init__ - : This RL environment uses a PPO RL algorithm agent in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\ppo_algorithm.py:49
2025-06-04 21:21:11,932 - INFO - base_algorithms.py:hyper_parameters - : Training is using non-tuned hyperparameters... in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\base_algorithms.py:118
2025-06-04 21:21:11,932 - INFO - base_algorithms.py:hyper_parameters - : Hyperparameters are:
{} in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\base_algorithms.py:119
2025-06-04 21:21:14,164 - INFO - rl_algo_trading_agent.py:train - : Training RL algo trading agent with the RLAgorithmType.ppo algorithm in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_agents\rl_algo_trading_agent.py:39
2025-06-04 21:21:14,170 - INFO - save_on_base_reward_callback.py:_init_callback - : Saving trained RL model to ./logs/Pendulum-v1_ppo\best_model in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:47
2025-06-04 21:21:17,499 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 21:22:09,905 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 21:22:30,452 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 21:22:42,025 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 21:23:29,112 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 21:23:40,754 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 21:23:52,420 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 21:24:04,147 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 21:24:15,966 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
2025-06-04 21:25:02,453 - INFO - save_on_base_reward_callback.py:_on_step - : Saving new best model to ./logs/Pendulum-v1_ppo\best_model.zip in C:\Development\Python\AI\ReinforcementLearning\PyData2025-Presentation\demos\rl-robo-algo-trader\src\main\rl_algorithms\train_evaluate_test\save_on_base_reward_callback.py:68
